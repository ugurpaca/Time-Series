{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# When Simple Beats Complex: Classical Forecasting Methods\n\n*Part 4 of 8: Moving Averages & Exponential Smoothing*\n\nDavid's LSTM: 18% MAPE, 3 months of work, GPU clusters.  \nElena's exponential smoothing: 12% MAPE, 5 minutes, laptop.\n\nToday: When (and why) simple methods win.\n\n![Complexity Ladder](forecasting_complexity_ladder.png)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (16, 8)\nnp.random.seed(42)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## The Philosophy\n\n**Start simple. Add complexity only when simple fails.**\n\n1. Naive (baseline)\n2. Moving Average\n3. Exponential Smoothing\n4. Holt's Method (+trend)\n5. Holt-Winters (+seasonality)\n6. ARIMA (Part 5)\n7. Deep Learning (Part 8)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Generate sample data\nn = 200\ntrend = np.linspace(100, 120, n)\nnoise = np.random.normal(0, 5, n)\ndata = trend + noise + np.cumsum(np.random.normal(0, 2, n))\n\ndates = pd.date_range('2023-01-01', periods=n, freq='D')\ndf = pd.DataFrame({'value': data}, index=dates)\n\n# Split\ntrain_size = int(0.8 * len(df))\ntrain, test = df[:train_size], df[train_size:]\n\nprint(f\"Train: {len(train)}, Test: {len(test)}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Method 1: Naive Forecast\n\n**Formula**: \u0177(t+1) = y(t)\n\nYour baseline. Every method must beat this."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Naive forecast\nnaive_forecast = [train['value'].iloc[-1]] * len(test)\n\nmae = mean_absolute_error(test['value'], naive_forecast)\nmape = mean_absolute_percentage_error(test['value'], naive_forecast)\n\nprint(f\"Naive Forecast - MAPE: {mape*100:.2f}%\")\n\nplt.figure(figsize=(16, 6))\nplt.plot(train.index, train['value'], label='Train', linewidth=2)\nplt.plot(test.index, test['value'], label='Actual', linewidth=2)\nplt.plot(test.index, naive_forecast, label='Naive', linewidth=2, linestyle='--')\nplt.axvline(x=train.index[-1], color='red', linestyle=':', alpha=0.5)\nplt.title('Naive Forecast', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Method 2: Simple Moving Average\n\n**Formula**: \u0177(t+1) = (1/N) \u00d7 \u03a3 y(t-i)\n\nSmooths noise by averaging."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def simple_moving_average(data, window):\n    return data[-window:].mean()\n\n# Try different windows\nwindows = [3, 7, 14]\nfor window in windows:\n    forecasts = []\n    for i in range(len(test)):\n        history = pd.concat([train['value'], test['value'].iloc[:i]])\n        forecast = simple_moving_average(history, window)\n        forecasts.append(forecast)\n    \n    mape = mean_absolute_percentage_error(test['value'], forecasts)\n    print(f\"SMA({window}): MAPE = {mape*100:.2f}%\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Method 3: Exponential Smoothing\n\n![Alpha Effect](alpha_parameter_effect.png)\n\n**Formula**: \u0177(t+1) = \u03b1\u00b7y(t) + (1-\u03b1)\u00b7\u0177(t)\n\nRecent data matters more."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Simple Exponential Smoothing\nses_model = SimpleExpSmoothing(train['value'])\nses_fit = ses_model.fit(optimized=True)\nses_forecast = ses_fit.forecast(steps=len(test))\n\nmape_ses = mean_absolute_percentage_error(test['value'], ses_forecast)\nprint(f\"\\nSES: MAPE = {mape_ses*100:.2f}%\")\nprint(f\"Optimized \u03b1 = {ses_fit.params['smoothing_level']:.3f}\")\n\nplt.figure(figsize=(16, 6))\nplt.plot(train.index, train['value'], label='Train', linewidth=2)\nplt.plot(test.index, test['value'], label='Actual', linewidth=2)\nplt.plot(test.index, ses_forecast, label='SES', linewidth=2, linestyle='--')\nplt.axvline(x=train.index[-1], color='red', linestyle=':', alpha=0.5)\nplt.title(f'Simple Exponential Smoothing - MAPE: {mape_ses*100:.2f}%', \n         fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Method 4: Holt's Method (Double ES)\n\nAdds **trend** component.\n\n**Use when**: Data has trend but no seasonality."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Generate trending data\nn = 200\ntrend_data = np.linspace(100, 200, n) + np.random.normal(0, 10, n)\ndf_trend = pd.DataFrame({'value': trend_data}, \n                        index=pd.date_range('2023-01-01', periods=n, freq='D'))\n\ntrain_trend = df_trend[:int(0.8*len(df_trend))]\ntest_trend = df_trend[int(0.8*len(df_trend)):]\n\n# Holt's method\nholt_model = Holt(train_trend['value'])\nholt_fit = holt_model.fit(optimized=True)\nholt_forecast = holt_fit.forecast(steps=len(test_trend))\n\nmape_holt = mean_absolute_percentage_error(test_trend['value'], holt_forecast)\nprint(f\"\\nHolt's Method: MAPE = {mape_holt*100:.2f}%\")\nprint(f\"\u03b1 (level) = {holt_fit.params['smoothing_level']:.3f}\")\nprint(f\"\u03b2 (trend) = {holt_fit.params['smoothing_trend']:.3f}\")\n\nplt.figure(figsize=(16, 6))\nplt.plot(train_trend.index, train_trend['value'], label='Train', linewidth=2)\nplt.plot(test_trend.index, test_trend['value'], label='Actual', linewidth=2)\nplt.plot(test_trend.index, holt_forecast, label=\"Holt's\", linewidth=2, linestyle='--')\nplt.title(f\"Holt's Method - MAPE: {mape_holt*100:.2f}%\", fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Method 5: Holt-Winters (Triple ES)\n\nHandles **level + trend + seasonality**.\n\nElena's secret weapon."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Generate seasonal data\nn = 365 * 2\ntime = np.arange(n)\ntrend = 100 + 0.1 * time\nseasonal = 30 * np.sin(2 * np.pi * time / 365)\nnoise = np.random.normal(0, 5, n)\nseasonal_data = trend + seasonal + noise\n\ndf_seasonal = pd.DataFrame({'value': seasonal_data},\n                           index=pd.date_range('2022-01-01', periods=n, freq='D'))\n\ntrain_seasonal = df_seasonal[:int(0.8*len(df_seasonal))]\ntest_seasonal = df_seasonal[int(0.8*len(df_seasonal)):]\n\n# Holt-Winters\nhw_model = ExponentialSmoothing(\n    train_seasonal['value'],\n    seasonal_periods=365,\n    trend='add',\n    seasonal='add'\n)\nhw_fit = hw_model.fit(optimized=True)\nhw_forecast = hw_fit.forecast(steps=len(test_seasonal))\n\nmape_hw = mean_absolute_percentage_error(test_seasonal['value'], hw_forecast)\nprint(f\"\\nHolt-Winters: MAPE = {mape_hw*100:.2f}%\")\n\nplt.figure(figsize=(16, 6))\nplt.plot(train_seasonal.index, train_seasonal['value'], \n        label='Train', linewidth=1.5, alpha=0.7)\nplt.plot(test_seasonal.index, test_seasonal['value'], \n        label='Actual', linewidth=2)\nplt.plot(test_seasonal.index, hw_forecast, \n        label='Holt-Winters', linewidth=2, linestyle='--')\nplt.title(f'Holt-Winters - MAPE: {mape_hw*100:.2f}%', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Complete Pipeline\n\n![Decision Tree](classical_methods_decision_tree.png)\n\nElena's winning strategy:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def forecast_pipeline(train, test, seasonal_period=None):\n    results = {}\n    \n    # 1. Naive\n    naive = [train.iloc[-1]] * len(test)\n    results['Naive'] = mean_absolute_percentage_error(test, naive)\n    \n    # 2. SES\n    ses = SimpleExpSmoothing(train).fit(optimized=True)\n    results['SES'] = mean_absolute_percentage_error(test, ses.forecast(len(test)))\n    \n    # 3. Holt\n    holt = Holt(train).fit(optimized=True)\n    results['Holt'] = mean_absolute_percentage_error(test, holt.forecast(len(test)))\n    \n    # 4. Holt-Winters (if seasonal)\n    if seasonal_period:\n        hw = ExponentialSmoothing(train, seasonal_periods=seasonal_period,\n                                  trend='add', seasonal='add').fit(optimized=True)\n        results['HW'] = mean_absolute_percentage_error(test, hw.forecast(len(test)))\n    \n    # Results\n    print(\"\\nMethod Comparison:\")\n    for name, mape in sorted(results.items(), key=lambda x: x[1]):\n        print(f\"{name:12s}: {mape*100:6.2f}%\")\n    \n    return results\n\n# Run pipeline\nresults = forecast_pipeline(train_seasonal['value'], test_seasonal['value'], 365)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Key Takeaways\n\n1. **Start simple** - Naive is your baseline\n2. **Add complexity incrementally** - Don't skip steps\n3. **Classical often wins** - Especially with limited data\n4. **Understand parameters**:\n   - \u03b1: Level smoothing (0.8-0.9 for responsive)\n   - \u03b2: Trend smoothing (0.1-0.2 typical)\n   - \u03b3: Seasonal smoothing (auto-optimize)\n5. **Check residuals** - They tell you what's missing\n\n## When Classical Beats Deep Learning\n\n- Limited data (<1000 points)\n- Clear patterns (trend/seasonality)\n- Need interpretability\n- Real-time requirements\n- Standard hardware\n\n## What's Next\n\n**Part 5: ARIMA Models** - Mathematical rigor for complex patterns.\n\nThe series so far:\n1. Fundamentals \u2713\n2. Decomposition \u2713\n3. Stationarity \u2713\n4. Classical Methods \u2713\n5. ARIMA \u2192 Next!"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}